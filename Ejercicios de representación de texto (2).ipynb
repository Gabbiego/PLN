{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Actividad 7 – Ejercicios de Representación de Texto"
      ],
      "metadata": {
        "id": "ZBTeJnjv_RlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 1"
      ],
      "metadata": {
        "id": "L9_bg48c_Xx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "corpus_es = [\n",
        "    \"Me encantó la película, los actores fueron increíbles\",\n",
        "    \"No me gustó la película, el guion fue muy malo\",\n",
        "    \"La actuación fue excelente, pero la historia era predecible\",\n",
        "    \"Película aburrida, me dormí a la mitad\",\n",
        "    \"¡Maravillosa! Recomiendo esta película a todos mis amigos\",\n",
        "    \"El guion estaba mal escrito, pero la actuación salvó la película\",\n",
        "    \"Demasiado larga y lenta, no la volvería a ver\",\n",
        "    \"Me gustó mucho, los efectos especiales fueron impresionantes\",\n",
        "    \"No es mala, pero esperaba algo más emocionante\",\n",
        "    \"Una obra maestra del cine español, increíble dirección\"\n",
        "]\n",
        "\n",
        "def primeros_tokens_indices(vectorizer, n=10):\n",
        "    vocab = vectorizer.vocabulary_\n",
        "    return sorted(vocab.items(), key=lambda kv: kv[1])[:n]\n",
        "\n",
        "# --- CountVectorizer crudo ---\n",
        "cv = CountVectorizer()\n",
        "X_cv = cv.fit_transform(corpus_es)\n",
        "print(\"=== Ejercicio 1 - CountVectorizer (crudo) ===\")\n",
        "print(\"Dimensiones:\", X_cv.shape)\n",
        "print(\"Primeros 10 tokens+índices:\", primeros_tokens_indices(cv, 10))\n",
        "print()\n",
        "\n",
        "# --- TF-IDF crudo ---\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(corpus_es)\n",
        "print(\"=== Ejercicio 1 - TF-IDF (crudo) ===\")\n",
        "print(\"Dimensiones:\", X_tfidf.shape)\n",
        "print(\"Primeros 10 tokens+índices:\", primeros_tokens_indices(tfidf, 10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wBcXtNoZBjHF",
        "outputId": "7d9908b7-1bc4-4060-8416-d48a60ed5328"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Ejercicio 1 - CountVectorizer (crudo) ===\n",
            "Dimensiones: (10, 57)\n",
            "Primeros 10 tokens+índices: [('aburrida', 0), ('actores', 1), ('actuación', 2), ('algo', 3), ('amigos', 4), ('cine', 5), ('del', 6), ('demasiado', 7), ('dirección', 8), ('dormí', 9)]\n",
            "\n",
            "=== Ejercicio 1 - TF-IDF (crudo) ===\n",
            "Dimensiones: (10, 57)\n",
            "Primeros 10 tokens+índices: [('aburrida', 0), ('actores', 1), ('actuación', 2), ('algo', 3), ('amigos', 4), ('cine', 5), ('del', 6), ('demasiado', 7), ('dirección', 8), ('dormí', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords en español (LISTA)\n",
        "spanish_stopwords = [\n",
        "    \"a\",\"acá\",\"ahí\",\"al\",\"algo\",\"algunas\",\"algunos\",\"allá\",\"alli\",\"ante\",\"antes\",\"aqui\",\"así\",\n",
        "    \"aún\",\"cada\",\"como\",\"con\",\"contra\",\"cual\",\"cuales\",\"cuando\",\"de\",\"del\",\"desde\",\"donde\",\n",
        "    \"dos\",\"el\",\"ella\",\"ellas\",\"ellos\",\"en\",\"entre\",\"era\",\"erais\",\"eran\",\"eras\",\"eres\",\"es\",\n",
        "    \"esa\",\"esas\",\"ese\",\"eso\",\"esos\",\"esta\",\"estaba\",\"estaban\",\"estamos\",\"estan\",\"estar\",\n",
        "    \"estas\",\"este\",\"esto\",\"estos\",\"estoy\",\"fue\",\"fueron\",\"han\",\"hasta\",\"hay\",\"la\",\"las\",\"le\",\n",
        "    \"les\",\"lo\",\"los\",\"más\",\"me\",\"mi\",\"mis\",\"muy\",\"ni\",\"no\",\"nos\",\"nosotros\",\"o\",\"otra\",\"otros\",\n",
        "    \"para\",\"pero\",\"poco\",\"por\",\"porque\",\"que\",\"se\",\"sea\",\"ser\",\"si\",\"sí\",\"sin\",\"sobre\",\"su\",\n",
        "    \"sus\",\"tal\",\"también\",\"tan\",\"te\",\"tenemos\",\"tiene\",\"tienen\",\"toda\",\"todo\",\"todos\",\"tu\",\n",
        "    \"tus\",\"un\",\"una\",\"uno\",\"unos\",\"usted\",\"ustedes\",\"y\",\"ya\"\n",
        "]\n",
        "\n",
        "# --- CountVectorizer pre ---\n",
        "cv2 = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    strip_accents='unicode',\n",
        "    stop_words=spanish_stopwords\n",
        ")\n",
        "X_cv2 = cv2.fit_transform(corpus_es)\n",
        "print(\"=== Ejercicio 2 - CountVectorizer (preprocesado) ===\")\n",
        "print(\"Dimensiones:\", X_cv2.shape)\n",
        "print(\"Primeros 10 tokens+índices:\", primeros_tokens_indices(cv2, 10))\n",
        "print()\n",
        "\n",
        "# --- TF-IDF pre ---\n",
        "tfidf2 = TfidfVectorizer(\n",
        "    lowercase=True,\n",
        "    strip_accents='unicode',\n",
        "    stop_words=spanish_stopwords\n",
        ")\n",
        "X_tfidf2 = tfidf2.fit_transform(corpus_es)\n",
        "print(\"=== Ejercicio 2 - TF-IDF (preprocesado) ===\")\n",
        "print(\"Dimensiones:\", X_tfidf2.shape)\n",
        "print(\"Primeros 10 tokens+índices:\", primeros_tokens_indices(tfidf2, 10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "93Q32yFkBnsM",
        "outputId": "f7c05a3f-4ba0-40ed-a077-7b988da034b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Ejercicio 2 - CountVectorizer (preprocesado) ===\n",
            "Dimensiones: (10, 39)\n",
            "Primeros 10 tokens+índices: [('aburrida', 0), ('actores', 1), ('actuacion', 2), ('amigos', 3), ('cine', 4), ('demasiado', 5), ('direccion', 6), ('dormi', 7), ('efectos', 8), ('emocionante', 9)]\n",
            "\n",
            "=== Ejercicio 2 - TF-IDF (preprocesado) ===\n",
            "Dimensiones: (10, 39)\n",
            "Primeros 10 tokens+índices: [('aburrida', 0), ('actores', 1), ('actuacion', 2), ('amigos', 3), ('cine', 4), ('demasiado', 5), ('direccion', 6), ('dormi', 7), ('efectos', 8), ('emocionante', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusiones\n",
        "\n",
        "En el ejercicio 1, al usar el texto sin procesar, la matriz quedó con un vocabulario de aproximadamente 57 términos, en el que se incluían artículos, pronombres y otras palabras poco útiles. CountVectorizer mostró solo las frecuencias de estos términos y TF-IDF ajustó sus pesos, pero el ruido todavía estaba presente.\n",
        "\n",
        "En el ejercicio 2, después del preprocesamiento (minúsculas, normalización de acentos y eliminación de stopwords en español), el vocabulario se redujo a unos 39 términos. Esto permitió eliminar gran parte del ruido y conservar palabras más representativas como *guion*, *actuacion* o *efectos*. En este escenario, CountVectorizer ya ofreció resultados más claros y TF-IDF resaltó mejor las diferencias entre reseñas.\n",
        "\n",
        "En conjunto, el preprocesamiento resultó clave para obtener representaciones más limpias y prácticas. La alternativa más útil es **TF-IDF con preprocesamiento**, porque concentra la información en los términos relevantes y descarta lo que no aporta valor.\n"
      ],
      "metadata": {
        "id": "UUEvXBMJGHIi"
      }
    }
  ]
}